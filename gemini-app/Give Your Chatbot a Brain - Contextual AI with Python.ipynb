{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Give Your Chatbot a Brain - Contextual AI with Python\n",
    "## Introduction\n",
    "\n",
    "Imagine a conversation that feels more like talking to a friend and less like interacting with a machine. That's the power of a chatbot with a memory! In today's digital world, chatbots are everywhere, from answering customer service questions to providing companionship. But what truly sets a great chatbot apart? The ability to remember past conversations and tailor responses accordingly. This is where the magic of contextual chatbots comes in.\n",
    "\n",
    "The most of popular LLM already supports contextual chatbot by including chat history, such as OpenAI and Gemini Pro, but not all of them. As education purpose, we implment a simple chat history chatbot using Gemini Pro.\n",
    "\n",
    "This tutorial will guide you through building a secure and contextual Gemini chatbot using Python. We'll leverage the power of Google's generative AI library (google.generativeai) to create chat sessions that chatbot will remember. By incorporating memory into your chatbot, you'll design a more engaging and helpful experience.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Gemini chat hostory demo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import google.generativeai as genai\n",
    "import os\n",
    "import datetime\n",
    "from dotenv import load_dotenv\n",
    "from os.path import expanduser\n",
    "\n",
    "\n",
    "envpath = '~'\n",
    "envfile = '.env'\n",
    "\n",
    "if envpath == '~':\n",
    "    envpath = os.path.expanduser(\"~\")\n",
    "\n",
    "load_dotenv(os.path.join(envpath, envfile))\n",
    "\n",
    "genai.configure(api_key=os.getenv(\"GOOGLE_API_KEY\"))\n",
    "\n",
    "modelname = \"gemini-1.5-flash\"\n",
    "model = genai.GenerativeModel(modelname)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Why did the AI get fired from its job at the library? \n",
      "\n",
      "Because it was always getting lost in the stacks! üìöü§ñ \n",
      "\n",
      "Why did the AI cross the road? \n",
      "\n",
      "To get to the other *side* of the algorithm! ü§ñ \n",
      "\n"
     ]
    }
   ],
   "source": [
    "chat = model.start_chat(history=[])\n",
    "\n",
    "response = chat.send_message('tell me a joke about AI')\n",
    "print(response.text) \n",
    "\n",
    "response = chat.send_message('tell me another joke about AI')\n",
    "print(response.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[parts {\n",
      "  text: \"tell me a joke about AI\"\n",
      "}\n",
      "role: \"user\"\n",
      ", parts {\n",
      "  text: \"Why did the AI get fired from its job at the library? \\n\\nBecause it was always getting lost in the stacks! üìöü§ñ \\n\"\n",
      "}\n",
      "role: \"model\"\n",
      ", parts {\n",
      "  text: \"tell me a joke about AI\"\n",
      "}\n",
      "role: \"user\"\n",
      ", parts {\n",
      "  text: \"Why did the AI cross the road? \\n\\nTo get to the other *side* of the algorithm! ü§ñ \\n\"\n",
      "}\n",
      "role: \"model\"\n",
      "]\n"
     ]
    }
   ],
   "source": [
    "print(chat.history)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From below test, we can see Gemini has memory of previous conversatoin."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "You're right! I'm still under development, and I sometimes forget things like past conversations.  \n",
      "\n",
      "The first joke I told you was: \n",
      "\n",
      "\"Why did the AI get fired from its job at the library? Because it was always getting lost in the stacks!\" üìöü§ñ \n",
      "\n",
      "Let me know if you'd like to hear another one! üòÑ \n",
      "\n"
     ]
    }
   ],
   "source": [
    "response = chat.send_message('what was 1st joke you told me?')\n",
    "\n",
    "print(response.text) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "text: \"tell me a joke about AI\"\n",
      "\n",
      "text: \"Why did the AI get fired from its job at the library? \\n\\nBecause it was always getting lost in the stacks! üìöü§ñ \\n\"\n",
      "\n",
      "text: \"tell me a joke about AI\"\n",
      "\n",
      "text: \"Why did the AI cross the road? \\n\\nTo get to the other *side* of the algorithm! ü§ñ \\n\"\n",
      "\n",
      "text: \"what was 1st joke you told me?\"\n",
      "\n",
      "text: \"You\\'re right! I\\'m still under development, and I sometimes forget things like past conversations.  \\n\\nThe first joke I told you was: \\n\\n\\\"Why did the AI get fired from its job at the library? Because it was always getting lost in the stacks!\\\" üìöü§ñ \\n\\nLet me know if you\\'d like to hear another one! üòÑ \\n\"\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for part in (chat.history):\n",
    "    print(part.parts[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's create chat history feature without using default Gemini history[], this concept and approach will aplly for any LLM."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Building the Chatbot Class\n",
    "\n",
    "### Class Definition and Initialization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 212,
   "metadata": {},
   "outputs": [],
   "source": [
    "import google.generativeai as genai\n",
    "import os\n",
    "import datetime\n",
    "from dotenv import load_dotenv\n",
    "from os.path import expanduser\n",
    "\n",
    "class GeminiChatbot:\n",
    "    def __init__(self):\n",
    "        # Load the .env file from the home directory\n",
    "        self.envpath = '~'\n",
    "        self.envfile = '.env'\n",
    "\n",
    "        if self.envpath == '~':\n",
    "            self.envpath = os.path.expanduser(\"~\")\n",
    "        \n",
    "        load_dotenv(os.path.join(self.envpath, self.envfile))\n",
    "\n",
    "        genai.configure(api_key=os.getenv(\"GOOGLE_API_KEY\"))\n",
    "\n",
    "        self.modelname = \"gemini-1.5-flash\"\n",
    "        self.model = genai.GenerativeModel(self.modelname)\n",
    "        \n",
    "        self.chat_history = []\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* **Class Definition:** The `GeminiChatbot` class encapsulates the chatbot's functionality.\n",
    "* **.env File Loading:** By default, the `load_dotenv` function loads the `.env` file from the home directory, securely storing the API key. You can change to any envpath and envfile.\n",
    "* **API Key Configuration:** The `os.getenv(\"GOOGLE_API_KEY\")` retrieves the API key from the environment variable and configures the Gemini API.\n",
    "* **Model Initialization:** By default, a `GenerativeModel` instance is created using the `gemini-1.5-flash` model, you can change to any Gemini model.\n",
    "* **Chat History Initialization:** An empty list is created to store the chat history."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 213,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Why did the AI get fired from the dating app?\n",
      "\n",
      "Because it couldn't tell the difference between a \"like\" and a \"dislike,\" and kept matching people with their worst enemies! \n",
      "\n",
      "['You: tell a joke about AI', 'Chatbot: Why did the AI get fired from the dating app?\\n\\nBecause it couldn\\'t tell the difference between a \"like\" and a \"dislike,\" and kept matching people with their worst enemies! \\n']\n"
     ]
    }
   ],
   "source": [
    "chatbot = GeminiChatbot()\n",
    "user_input = (\"tell a joke about AI\")\n",
    "chatbot.chat_history.append(f\"You: {user_input}\")\n",
    "response = chatbot.model.generate_content(user_input)\n",
    "chatbot.chat_history.append(f\"Chatbot: {response.text}\")\n",
    "print(response.text)\n",
    "print(chatbot.chat_history)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generating Response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 223,
   "metadata": {},
   "outputs": [],
   "source": [
    "import google.generativeai as genai\n",
    "import os\n",
    "import datetime\n",
    "from dotenv import load_dotenv\n",
    "from os.path import expanduser\n",
    "\n",
    "class GeminiChatbot:\n",
    "    def __init__(self):\n",
    "        # Load the .env file from the home directory\n",
    "        self.envpath = '~'\n",
    "        self.envfile = '.env'\n",
    "\n",
    "        if self.envpath == '~':\n",
    "            self.envpath = os.path.expanduser(\"~\")\n",
    "        \n",
    "        load_dotenv(os.path.join(self.envpath, self.envfile))\n",
    "\n",
    "        genai.configure(api_key=os.getenv(\"GOOGLE_API_KEY\"))\n",
    "\n",
    "        self.modelname = \"gemini-1.5-flash\"\n",
    "        self.model = genai.GenerativeModel(self.modelname)\n",
    "        \n",
    "        self.chat_history = []\n",
    "\n",
    "    def generate_response(self, prompt):\n",
    "        full_prompt = \"Please go through chat history below if user ask question regarding on previous conversation.\\nPlease anwser question directly if it is not related to previous conversation\\n\" + \"+++chat history\\n\" + ''.join(self.chat_history) + \"+++\\n\" + \"new prompt: \" + prompt\n",
    "        response = self.model.generate_content(full_prompt)\n",
    "        return response.text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* The `generate_response` method takes a user prompt as input.\n",
    "* The full combined prompt is concatenated with the chat history lookup rule, entire chat history and the current prompt to provide context for the response.\n",
    "* The Gemini model generates a response based on the combined prompt."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 224,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Why did the AI cross the road? \n",
      "\n",
      "To get to the other *side* of the algorithm! \n",
      "\n"
     ]
    }
   ],
   "source": [
    "chatbot = GeminiChatbot()\n",
    "user_input = (\"tell a joke about AI\")\n",
    "chatbot.chat_history.append(f\"You: {user_input}\")\n",
    "response = chatbot.generate_response(user_input)\n",
    "chatbot.chat_history.append(f\"Chatbot: {response}\")\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 225,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The joke I told you was:\n",
      "\n",
      "Why did the AI cross the road? \n",
      "\n",
      "To get to the other *side* of the algorithm! \n",
      "\n"
     ]
    }
   ],
   "source": [
    "user_input = (\"what was joke you told me?\")\n",
    "chatbot.chat_history.append(f\"You: {user_input}\")\n",
    "response = chatbot.generate_response(user_input)\n",
    "chatbot.chat_history.append(f\"Chatbot: {response}\")\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 226,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['You: tell a joke about AI', 'Chatbot: Why did the AI cross the road? \\n\\nTo get to the other *side* of the algorithm! \\n', 'You: what was joke you told me?', 'Chatbot: The joke I told you was:\\n\\nWhy did the AI cross the road? \\n\\nTo get to the other *side* of the algorithm! \\n']\n"
     ]
    }
   ],
   "source": [
    "print(chatbot.chat_history)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Logging Chat History"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 227,
   "metadata": {},
   "outputs": [],
   "source": [
    "import google.generativeai as genai\n",
    "import os\n",
    "import datetime\n",
    "from dotenv import load_dotenv\n",
    "from os.path import expanduser\n",
    "\n",
    "class GeminiChatbot:\n",
    "    def __init__(self):\n",
    "        # Load the .env file from the home directory\n",
    "        self.envpath = '~'\n",
    "        self.envfile = '.env'\n",
    "\n",
    "        if self.envpath == '~':\n",
    "            self.envpath = os.path.expanduser(\"~\")\n",
    "        \n",
    "        load_dotenv(os.path.join(self.envpath, self.envfile))\n",
    "\n",
    "        genai.configure(api_key=os.getenv(\"GOOGLE_API_KEY\"))\n",
    "\n",
    "        self.modelname = \"gemini-1.5-flash\"\n",
    "        self.model = genai.GenerativeModel(self.modelname)\n",
    "        \n",
    "        self.chat_history = []\n",
    "\n",
    "    def generate_response(self, prompt):\n",
    "        full_prompt = \"Please go through chat history below if user ask question regarding on previous conversation.\\nPlease anwser question directly if it is not related to previous conversation\\n\" + \"+++chat history\\n\" + ''.join(self.chat_history) + \"+++\\n\" + \"new prompt: \" + prompt\n",
    "        response = self.model.generate_content(full_prompt)\n",
    "        return response.text\n",
    "\n",
    "    def log_chat_history(self,logpath):\n",
    "        timestamp = datetime.datetime.now().strftime(\"%Y-%m-%d_%H-%M-%S\")\n",
    "        log_filename = f\"chat-log-{timestamp}.txt\"\n",
    "        log_path = os.path.join(logpath, log_filename)\n",
    "\n",
    "        os.makedirs(logpath, exist_ok=True)\n",
    "\n",
    "        with open(log_path, \"w\") as f:\n",
    "            for message in self.chat_history:\n",
    "                f.write(f\"{message}\\n\")\n",
    "        print(f\"chat log file: {log_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* This method logs the chat history to a text file.\n",
    "* A timestamp is generated to create a unique filename for the log.\n",
    "* The log directory is created if it doesn't exist.\n",
    "* The chat history is written to the log file, one message per line."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 228,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Why did the AI cross the road? \n",
      "\n",
      "To get to the other *side* of the algorithm! \n",
      "\n",
      "I'm doing well, thank you for asking! How about you? üòä \n",
      "\n",
      "You asked: \"Why did the AI cross the road?\" \n",
      "\n",
      "And I answered: \"To get to the other *side* of the algorithm!\" \n",
      "\n",
      "chat log file: ./log/chat-log-2024-08-18_12-54-43.txt\n"
     ]
    }
   ],
   "source": [
    "chatbot = GeminiChatbot()\n",
    "user_input = \"tell a joke about AI\"\n",
    "chatbot.chat_history.append(f\"You: {user_input}\")\n",
    "response = chatbot.generate_response(user_input)\n",
    "chatbot.chat_history.append(f\"Chatbot: {response}\")\n",
    "print(response)\n",
    "\n",
    "user_input = \"how are you today?\"\n",
    "chatbot.chat_history.append(f\"You: {user_input}\")\n",
    "response = chatbot.generate_response(user_input)\n",
    "chatbot.chat_history.append(f\"Chatbot: {response}\")\n",
    "print(response)\n",
    "\n",
    "user_input = \"what was joke you told me?\"\n",
    "chatbot.chat_history.append(f\"You: {user_input}\")\n",
    "response = chatbot.generate_response(user_input)\n",
    "chatbot.chat_history.append(f\"Chatbot: {response}\")\n",
    "print(response)\n",
    "\n",
    "chatbot.log_chat_history('./log')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 229,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['You: tell a joke about AI',\n",
       " 'Chatbot: Why did the AI cross the road? \\n\\nTo get to the other *side* of the algorithm! \\n',\n",
       " 'You: how are you today?',\n",
       " \"Chatbot: I'm doing well, thank you for asking! How about you? üòä \\n\",\n",
       " 'You: what was joke you told me?',\n",
       " 'Chatbot: You asked: \"Why did the AI cross the road?\" \\n\\nAnd I answered: \"To get to the other *side* of the algorithm!\" \\n']"
      ]
     },
     "execution_count": 229,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chatbot.chat_history"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Using the Chatbot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "    def chat(self):\n",
    "        print(\"Welcome to GeminiChatbot! ('/q' to exit)\\n\")\n",
    "        self.chat_history.append(\"Welcome to GeminiChatbot! ('/q' to exit)\\n\")\n",
    "        while True:\n",
    "            user_input = input(\"You: \")\n",
    "            self.chat_history.append(f\"You: {user_input}\\n\")\n",
    "                      \n",
    "            if user_input.lower() == \"/q\":\n",
    "                self.log_chat_history()\n",
    "                print(\"Chat history saved. Exiting.\")\n",
    "                break\n",
    "            response = self.generate_response(user_input)\n",
    "            print(f\"Chatbot: {response}\")\n",
    "            self.chat_history.append(f\"Chatbot: {response}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The chat function is main chatbot to end user:\n",
    "* It enters a loop to continuously prompt the user for input.\n",
    "* The chatbot generates responses based on the user's input and the chat history.\n",
    "* When the user enters \"/q\", the chat history is logged and the program exits."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Complete gemini chatbot app"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 220,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Welcome to GeminiChatbot! ('/q' to exit)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 You:  hello\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 Chatbot: Hello! üëã How can I help you today? \n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2 You:  how are you?\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2 Chatbot: I'm doing well, thank you for asking! How can I help you today? üòä \n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3 You:  tell me a joke\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3 Chatbot: Why don't scientists trust atoms?\n",
      "\n",
      "Because they make up everything! \n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4 You:  tell me another joke\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4 Chatbot: Why don't they play poker in the jungle? \n",
      "\n",
      "Too many cheetahs! üêÜ \n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5 You:  what was 1st joke?\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5 Chatbot: The first joke was:  \"Why don't scientists trust atoms? Because they make up everything!\" \n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6 You:  what was 2nd joke?\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6 Chatbot: The second joke was: \"Why don't they play poker in the jungle? Too many cheetahs!\" üêÜ \n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7 You:  what was 1st word I said?\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7 Chatbot: The first word you said was \"hello\". \n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8 You:  what was 2nd word I said?\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8 Chatbot: The second word you said was \"how\". \n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9 You:  what was 2nd question I asked?\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9 Chatbot: The second question you asked was \"how are you?\" \n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10 You:  /q\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "chat log file: ./log/chat-log-2024-08-18_12-41-13.txt\n",
      "Chat history saved. Exiting.\n"
     ]
    }
   ],
   "source": [
    "import google.generativeai as genai\n",
    "import os\n",
    "import datetime\n",
    "from dotenv import load_dotenv\n",
    "from os.path import expanduser\n",
    "\n",
    "class GeminiChatbot:\n",
    "    def __init__(self):\n",
    "        # Load the .env file from the home directory\n",
    "        self.envpath = '~'\n",
    "        self.envfile = '.env'\n",
    "\n",
    "        if self.envpath == '~':\n",
    "            self.envpath = os.path.expanduser(\"~\")\n",
    "        \n",
    "        load_dotenv(os.path.join(self.envpath, self.envfile))\n",
    "\n",
    "        genai.configure(api_key=os.getenv(\"GOOGLE_API_KEY\"))\n",
    "\n",
    "        self.modelname = \"gemini-1.5-flash\"\n",
    "        self.model = genai.GenerativeModel(self.modelname)\n",
    "        \n",
    "        self.chat_history = []\n",
    "\n",
    "    def generate_response(self, prompt):\n",
    "        full_prompt = \"Please go through chat history below if user ask question regarding on previous conversation.\\nPlease anwser question directly if it is not related to previous conversation\\n\" + \"+++chat history\\n\" + ''.join(self.chat_history) + \"+++\\n\" + \"new prompt: \" + prompt\n",
    "        response = self.model.generate_content(full_prompt)\n",
    "        return response.text\n",
    "\n",
    "    def log_chat_history(self,logpath):\n",
    "        timestamp = datetime.datetime.now().strftime(\"%Y-%m-%d_%H-%M-%S\")\n",
    "        log_filename = f\"chat-log-{timestamp}.txt\"\n",
    "        log_path = os.path.join(logpath, log_filename)\n",
    "\n",
    "        os.makedirs(logpath, exist_ok=True)\n",
    "\n",
    "        with open(log_path, \"w\") as f:\n",
    "            for message in self.chat_history:\n",
    "                f.write(f\"{message}\\n\")\n",
    "        \n",
    "        print(f\"chat log file: {log_path}\")\n",
    "    \n",
    "    def chat(self):\n",
    "        n=1 # input number\n",
    "        print(\"Welcome to GeminiChatbot! ('/q' to exit)\\n\")\n",
    "        self.chat_history.append(\"Welcome to GeminiChatbot! ('/q' to exit)\\n\")\n",
    "        while True:\n",
    "            user_input = input(f\"{n} You: \")\n",
    "            self.chat_history.append(f\"{n} You: {user_input}\\n\")\n",
    "                      \n",
    "            if user_input.lower() == \"/q\":\n",
    "                self.log_chat_history('./log')\n",
    "                print(\"Chat history saved. Exiting.\")\n",
    "                break\n",
    "            response = self.generate_response(user_input)\n",
    "            print(f\"{n} Chatbot: {response}\")\n",
    "            self.chat_history.append(f\"{n} Chatbot: {response}\")\n",
    "            n += 1\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    chatbot = GeminiChatbot()\n",
    "    chatbot.chat()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 221,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[\"Welcome to GeminiChatbot! ('/q' to exit)\\n\",\n",
       " '1 You: hello\\n',\n",
       " '1 Chatbot: Hello! üëã How can I help you today? \\n',\n",
       " '2 You: how are you?\\n',\n",
       " \"2 Chatbot: I'm doing well, thank you for asking! How can I help you today? üòä \\n\",\n",
       " '3 You: tell me a joke\\n',\n",
       " \"3 Chatbot: Why don't scientists trust atoms?\\n\\nBecause they make up everything! \\n\",\n",
       " '4 You: tell me another joke\\n',\n",
       " \"4 Chatbot: Why don't they play poker in the jungle? \\n\\nToo many cheetahs! üêÜ \\n\",\n",
       " '5 You: what was 1st joke?\\n',\n",
       " '5 Chatbot: The first joke was:  \"Why don\\'t scientists trust atoms? Because they make up everything!\" \\n',\n",
       " '6 You: what was 2nd joke?\\n',\n",
       " '6 Chatbot: The second joke was: \"Why don\\'t they play poker in the jungle? Too many cheetahs!\" üêÜ \\n',\n",
       " '7 You: what was 1st word I said?\\n',\n",
       " '7 Chatbot: The first word you said was \"hello\". \\n',\n",
       " '8 You: what was 2nd word I said?\\n',\n",
       " '8 Chatbot: The second word you said was \"how\". \\n',\n",
       " '9 You: what was 2nd question I asked?\\n',\n",
       " '9 Chatbot: The second question you asked was \"how are you?\" \\n',\n",
       " '10 You: /q\\n']"
      ]
     },
     "execution_count": 221,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chatbot.chat_history"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Change chatbot instance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 222,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Welcome to GeminiChatbot! ('/q' to exit)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 You:  hello\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 Chatbot: Hello! üëã How can I help you today? üòä \n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2 You:  tell me a joke\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2 Chatbot: Why don't scientists trust atoms? \n",
      "\n",
      "Because they make up everything! \n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3 You:  tell me another joke\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3 Chatbot: Why don't they play poker in the jungle? \n",
      "\n",
      "Too many cheetahs! üêÜ \n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4 You:  what was 1st joke?\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4 Chatbot: The first joke was: \n",
      "\n",
      "Why don't scientists trust atoms? \n",
      "\n",
      "Because they make up everything! \n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5 You:  what was 2nd joke?\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5 Chatbot: The second joke was:\n",
      "\n",
      "Why don't they play poker in the jungle? \n",
      "\n",
      "Too many cheetahs! üêÜ \n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6 You:  what was 1st word I said?\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6 Chatbot: The first word you said was \"hello\". \n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7 You:  /q\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "chat log file: ./log/chat-log-2024-08-18_12-42-19.txt\n",
      "Chat history saved. Exiting.\n"
     ]
    }
   ],
   "source": [
    "chatbot = GeminiChatbot()\n",
    "chatbot.envpath = \"/mnt/c/dclab/dev/ai-ml\"\n",
    "chatbot.modelname = \"gemini-1.5-pro\"\n",
    "chatbot.chat()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "chatbot.chat_history"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusion\n",
    "\n",
    "Why is memory so important for chatbots? Just like real conversations, remembering past interactions allows chatbots to provide more relevant and personalized responses. In this blog, we've built a Gemini chatbot using Python that can summarize chat history and use that context to inform its responses. This approach leads to a more natural and engaging conversation for the user.\n",
    "\n",
    "By following these steps and using the google.generativeai library, you can create a robust and contextual Gemini chatbot that users will enjoy interacting with. Remember to replace 'GOOGLE_API_KEY' with your actual Gemini API key in the .env file. With this foundation, you can extend your chatbot's capabilities and create truly memorable chat experiences."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
