{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0c5ea3f4a75c"
   },
   "source": [
    "# Build RAG App using Gemini+LangChain+Chroma"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "479790a71f3c"
   },
   "source": [
    "## Overview\n",
    "\n",
    "[Gemini](https://ai.google.dev/models/gemini) - family of generative AI models used to generate content and solve problems; used to handle both text and images as input.\n",
    "\n",
    "[LangChain](https://www.langchain.com/) - data framework to integrate with Large Language Models (LLM) like Gemini easier for applications.\n",
    "\n",
    "[Chroma](https://docs.trychroma.com/) - open-source embedding database focused on simplicity and productivity; used to store embeddings and metadata, embed documents and queries, and search the embeddings quickly.\n",
    "\n",
    "Here is demo how to create a RAG application that answers questions using data from a website using Gemini, LangChain, and Chroma."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_qRjVe1tZhsx"
   },
   "source": [
    "## Installation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "olK4Ejjzuj76"
   },
   "outputs": [],
   "source": [
    "!pip install --quiet langchain langchain_community\n",
    "!pip install --quiet langchain-google-genai\n",
    "!pip install --quiet chromadb"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "wiGHSFmZaniK"
   },
   "source": [
    "## Setup API key\n",
    "\n",
    "Place below line to .env under current folder, \n",
    "\n",
    "GOOGLE_API_KEY=xxx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "xId4sR52utS0"
   },
   "outputs": [],
   "source": [
    "import google.generativeai as genai\n",
    "import os\n",
    "\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv()\n",
    "\n",
    "genai.configure(api_key=os.environ[\"GOOGLE_API_KEY\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "aEKMUyVmckWI"
   },
   "source": [
    "## RAG\n",
    "\n",
    "When making use of LLMs to answer questions based on private data, need to provide the relevant documents as context alongside your prompt. This is called Retrieval Augmented Generation (RAG).\n",
    "\n",
    "We can build a RAG app directly using Gemini API; but also can work through Langchain to make life more easier.\n",
    "\n",
    "In this demo, we implement the two main components in an RAG-based architecture:\n",
    "\n",
    "1. Retriever\n",
    "\n",
    "    Based on the user's query, the retriever retrieves relevant snippets that add context from the document (which is website data here)\n",
    "\n",
    "2. Generator\n",
    "\n",
    "    The relevant snippets from the website data are passed to the LLM along with the user's query to generate accurate answers."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "kPhs4mDkjdgY"
   },
   "source": [
    "## Import the required libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "TcvGPVdXu05F"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "USER_AGENT environment variable not set, consider setting it to identify your requests.\n"
     ]
    }
   ],
   "source": [
    "from langchain import PromptTemplate\n",
    "from langchain import hub\n",
    "from langchain.docstore.document import Document\n",
    "from langchain.document_loaders import WebBaseLoader\n",
    "from langchain.schema import StrOutputParser\n",
    "from langchain.schema.prompt_template import format_document\n",
    "from langchain.schema.runnable import RunnablePassthrough\n",
    "from langchain.vectorstores import Chroma"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4461Jihk_rWq"
   },
   "source": [
    "## Retriever\n",
    "\n",
    "Perform the following steps:\n",
    "\n",
    "- Read and parse the website data using LangChain.\n",
    "- Create embeddings of the website data.\n",
    "\n",
    "    Embeddings are numerical representations (vectors) of text. Hence, text with similar meaning will have similar embedding vectors. You'll make use of Gemini's embedding model to create the embedding vectors of the website data.\n",
    "\n",
    "- Store the embeddings in Chroma's vector store.\n",
    "    \n",
    "    Chroma is a vector database. The Chroma vector store helps in the efficient retrieval of similar vectors. Thus, for adding context to the prompt for the LLM, relevant embeddings of the text matching the user's question can be retrieved easily using Chroma.\n",
    "\n",
    "- Create a Retriever from the Chroma vector store.\n",
    "\n",
    "    The retriever will be used to pass relevant website embeddings to the LLM along with user queries."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "WomGvIAVjZeI"
   },
   "source": [
    "### Read and parse the website data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What is document format from web loader? - [Langchain WebBaseLoader](https://python.langchain.com/v0.2/docs/integrations/document_loaders/web_base/)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "id": "DeNX9QFM0V-C"
   },
   "outputs": [],
   "source": [
    "loader = WebBaseLoader(\"https://dreamcloud.artark.ca/build-k8s-cluster-on-wsl2/\")\n",
    "docs = loader.load()\n",
    "#docs[0]\n",
    "#docs[0].metadata ## dict\n",
    "#docs[0].page_content ## string"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "olIlIOYrJTlF"
   },
   "source": [
    "If you only want to select a specific portion of the website data to add context to the prompt, you can use regex, text slicing, or text splitting.\n",
    "\n",
    "We use split function to extract the required portion of the text. The extracted text should be converted back to LangChain's `Document` format.\n",
    "> `docs =  [Document(page_content=final_text, metadata={\"source\": \"local\"})]`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract the text from the website data document\n",
    "text_content = docs[0].page_content\n",
    "#text_content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "id": "EDL9YLRb9Bw2",
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "' in one shot using my handy script toolkit.\\nFirst of all, download it via git clone, or manually download from github.\\n\\r\\ngit clone git@github.com:robertluwang/hands-on-nativecloud.git\\r\\ncd ./hands-on-n'"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# split text after string\n",
    "text_content_1 = text_content.split(\"It is possible to install k8s installation\",1)[1]\n",
    "text_content_1[:200]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'-dockerd.sock \\r\\n\\r\\necho === $(date) Provisioning - k8s-reset.sh by $(whoami) end\\r\\n\\nFor example,\\n\\r\\nbash k8s-reset.sh\\r\\n\\nthen following k8s-init.sh with eth0 static ip,\\n\\r\\nbash k8s-init.sh 192.168.80.2\\r\\n\\n\\n'"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# split text before string\n",
    "final_text = text_content_1.split(\"k8s cluster test on WSL2\",1)[0]\n",
    "final_text[-200:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert the text to LangChain's `Document` format\n",
    "docs =  [Document(page_content=final_text, metadata={\"source\": \"local\"})]\n",
    "#docs[0].page_content"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "yDsdAg4Fjo5o"
   },
   "source": [
    "### Initialize Gemini's embedding model\n",
    "\n",
    "**embedding-001** supports creating text embeddings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "id": "8NXNTrjp0jdh"
   },
   "outputs": [],
   "source": [
    "from langchain_google_genai import GoogleGenerativeAIEmbeddings\n",
    "\n",
    "gemini_embeddings = GoogleGenerativeAIEmbeddings(model=\"models/embedding-001\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "m9Vzw30wpebs"
   },
   "source": [
    "### Store the data using Chroma\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "id": "n1VwhUQMvpcN"
   },
   "outputs": [],
   "source": [
    "# Save to disk\n",
    "vectorstore = Chroma.from_documents(\n",
    "                     documents=docs,                 # Data\n",
    "                     embedding=gemini_embeddings,    # Embedding model\n",
    "                     persist_directory=\"./chroma_db\" # Directory to save data\n",
    "                     )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "WFKyb3JXOeaQ"
   },
   "source": [
    "### Create a retriever using Chroma\n",
    "\n",
    "Create a retriever that can retrieve website data embeddings from the newly created Chroma vector store, it later used to pass embeddings that provide more context to the LLM for answering user's queries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {
    "id": "s3t4kmzIOZQq"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n"
     ]
    }
   ],
   "source": [
    "# Load from disk\n",
    "vectorstore_disk = Chroma(\n",
    "                        persist_directory=\"./chroma_db\",       # Directory of db\n",
    "                        embedding_function=gemini_embeddings   # Embedding model\n",
    "                   )\n",
    "retriever = vectorstore_disk.as_retriever(search_kwargs={\"k\": 1})\n",
    "\n",
    "print(len(retriever.get_relevant_documents(\"k8s\")))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "LZwcZyRxSO0q"
   },
   "source": [
    "## Generator\n",
    "\n",
    "The Generator prompts the LLM for an answer when the user asks a question. The retriever you created in the previous stage from the Chroma vector store will be used to pass relevant embeddings from the website data to the LLM to provide more context to the user's query.\n",
    "\n",
    "You'll perform the following steps in this stage:\n",
    "\n",
    "1. Chain together the following:\n",
    "    * A prompt for extracting the relevant embeddings using the retriever.\n",
    "    * A prompt for answering any question using LangChain.\n",
    "    * An LLM model from Gemini for prompting.\n",
    "    \n",
    "2. Run the created chain with a question as input to prompt the model for an answer.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "FtUi5FBIJMDy"
   },
   "source": [
    "### Initialize Gemini\n",
    "\n",
    "We use **gemini-pro** as it supports text summarization. \n",
    "\n",
    "Configure the model parameters such as ***temperature*** or ***top_p***,  by passing the appropriate values when initializing the `ChatGoogleGenerativeAI` LLM."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "id": "CaA1vRCh7s36"
   },
   "outputs": [],
   "source": [
    "from langchain_google_genai import ChatGoogleGenerativeAI\n",
    "\n",
    "llm = ChatGoogleGenerativeAI(model=\"gemini-pro\",\n",
    "                 temperature=0.7, top_p=0.85)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "jC4QDhiPpDJa"
   },
   "source": [
    "### Create prompt templates\n",
    "\n",
    "Use LangChain's [PromptTemplate](https://python.langchain.com/docs/modules/model_io/prompts/prompt_templates/) to generate prompts to the LLM for answering questions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "id": "90Czqh074dEC"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input_variables=['context', 'question'] template=\"You are an assistant for question-answering tasks.\\nUse the following context to answer the question.\\nIf you don't know the answer, just say that you don't know.\\nUse five sentences maximum and keep the answer concise.\\n\\nQuestion: {question} \\nContext: {context} \\nAnswer:\"\n"
     ]
    }
   ],
   "source": [
    "# Prompt template to query Gemini\n",
    "llm_prompt_template = \"\"\"You are an assistant for question-answering tasks.\n",
    "Use the following context to answer the question.\n",
    "If you don't know the answer, just say that you don't know.\n",
    "Use five sentences maximum and keep the answer concise.\\n\n",
    "Question: {question} \\nContext: {context} \\nAnswer:\"\"\"\n",
    "\n",
    "llm_prompt = PromptTemplate.from_template(llm_prompt_template)\n",
    "\n",
    "print(llm_prompt)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "KXDh2jsdp4sr"
   },
   "source": [
    "### Create a stuff documents chain\n",
    "\n",
    "LangChain provides [Chains](https://python.langchain.com/docs/modules/chains/) for chaining together LLMs with each other or other components for complex applications. \n",
    "\n",
    "The stuff documents chain for this application retrieves the relevant website data and passes it as the context to an LLM prompt along with the input question."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "id": "gj5sWzpwp7vc"
   },
   "outputs": [],
   "source": [
    "# Combine data from documents to readable string format.\n",
    "def format_docs(docs):\n",
    "    return \"\\n\\n\".join(doc.page_content for doc in docs)\n",
    "\n",
    "rag_chain = (\n",
    "    {\"context\": retriever | format_docs, \"question\": RunnablePassthrough()}\n",
    "    | llm_prompt\n",
    "    | llm\n",
    "    | StrOutputParser()\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "cPPqsGCLIrs1"
   },
   "source": [
    "### Prompt the model\n",
    "\n",
    "We can now query the LLM by passing any question to the `invoke()` function of the stuff documents chain created previously."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import display\n",
    "from IPython.display import Markdown\n",
    "import textwrap\n",
    "\n",
    "def to_markdown(text):\n",
    "  text = text.replace('•', '  *')\n",
    "  return Markdown(textwrap.indent(text, '> ', predicate=lambda _: True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {
    "id": "4vIaopCsIq0B"
   },
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "> The docker-server.sh script automates the installation of Docker on Ubuntu systems. It updates the system, installs required packages, adds the Docker repository, installs Docker, adds the current user to the Docker group, disables swap, configures Docker daemon settings, and restarts Docker.\n",
       "> \n",
       "> Here is the source code for the docker-server.sh script:\n",
       "> \n",
       "> ```bash\n",
       "> # docker-server.sh\n",
       "> # handy script to install docker on ubuntu \n",
       "> # run on k8s cluster node (master/worker)\n",
       "> # By Robert Wang @github.com/robertluwang\n",
       "> # Nov 21, 2022\n",
       "> \n",
       "> echo === $(date) Provisioning - docker-server.sh by $(whoami) start\n",
       "> \n",
       "> sudo apt-get update -y\n",
       "> sudo apt-get install -y ca-certificates curl gnupg lsb-release\n",
       "> curl -fsSL https://download.docker.com/linux/ubuntu/gpg | sudo gpg --dearmor -o /usr/share/keyrings/docker-archive-keyring.gpg\n",
       "> echo \"deb [arch=$(dpkg --print-architecture) signed-by=/usr/share/keyrings/docker-archive-keyring.gpg] https://download.docker.com/linux/ubuntu $(lsb_release -cs) stable\" | sudo tee /etc/apt/sources.list.d/docker.list > /dev/null\n",
       "> sudo apt-get update -y\n",
       "> sudo apt-get install -y docker-ce docker-ce-cli containerd.io \n",
       "> \n",
       "> sudo groupadd docker\n",
       "> sudo usermod -aG docker $USER\n",
       "> \n",
       "> # turn off swap\n",
       "> sudo swapoff -a\n",
       "> sudo sed -i '/swap/d' /etc/fstab\n",
       "> \n",
       "> sudo mkdir /etc/docker\n",
       "> cat <<EOF | sudo tee /etc/docker/daemon.json\n",
       "> {\n",
       ">   \"exec-opts\": [\"native.cgroupdriver=systemd\"],\n",
       ">   \"log-driver\": \"json-file\",\n",
       ">   \"log-opts\": {\n",
       ">     \"max-size\": \"100m\"\n",
       ">   },\n",
       ">   \"storage-driver\": \"overlay2\"\n",
       "> }\n",
       "> EOF\n",
       "> sudo systemctl enable docker\n",
       "> sudo systemctl daemon-reload\n",
       "> sudo systemctl restart docker\n",
       "> sleep 30 \n",
       "> sudo systemctl restart docker\n",
       "> \n",
       "> echo === $(date) Provisioning - docker-server.sh by $(whoami) end\n",
       "> ```"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "to_markdown(rag_chain.invoke(\"what is docker-server.sh for? show me source code\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see it analyzed the code then explain what is doing, also provide completely source code."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusion\n",
    "\n",
    "In this demo, we demonstrated how to build a Retrieval-Augmented Generation (RAG) application using Gemini, LangChain, and Chroma. By combining these powerful tools, we can efficiently retrieve and process relevant information from large datasets to provide accurate and concise answers to user queries.\n",
    "\n",
    "The process involved setting up the necessary libraries and API keys, parsing website data, creating embeddings using Gemini, and storing these embeddings in Chroma for efficient retrieval. The retriever fetched relevant snippets based on user queries, while the generator used these snippets to generate precise answers with the help of Gemini's language models.\n",
    "\n",
    "This approach highlights the synergy between advanced AI models and robust data frameworks, enabling the creation of intelligent applications that can handle complex information retrieval and generation tasks. Such applications can significantly enhance user experience by providing quick, accurate, and contextually relevant answers, making them invaluable tools in various domains, including customer support, research, and education."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "name": "Gemini_LangChain_QA_Chroma_WebLoad.ipynb",
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
